{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Log in to huggingface"
      ],
      "metadata": {
        "id": "GlxHqLbuhPIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "HF_TOKEN=userdata.get('HF_TOKEN')\n",
        "\n",
        "if HF_TOKEN:\n",
        "    login(HF_TOKEN)\n",
        "    print(\"Successfully logged in to Hugging Face!\")\n",
        "else:\n",
        "    print(\"Token is not set. Please save the token first.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0XIDj6ZhSvo",
        "outputId": "e692f05c-c318-44d2-bf65-1faa331a475c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully logged in to Hugging Face!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing prereqs"
      ],
      "metadata": {
        "id": "kTwExWqOhA2t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IndZS4Megnn5",
        "outputId": "27591727-2c8c-4bf6-9788-261fe0507a5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q bitsandbytes datasets accelerate loralib\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q peft transformers datasets"
      ],
      "metadata": {
        "id": "Sr6p25VNhMyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-stack -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wDp_hiFHhVH2",
        "outputId": "93f0c721-e529-4f1e-f880-aa960ab93602"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-stack\n",
            "  Downloading llama_stack-0.2.22-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from llama-stack) (3.12.15)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.0 in /usr/local/lib/python3.12/dist-packages (from llama-stack) (0.116.1)\n",
            "Collecting fire (from llama-stack)\n",
            "  Downloading fire-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from llama-stack) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from llama-stack) (0.34.4)\n",
            "Requirement already satisfied: jinja2>=3.1.6 in /usr/local/lib/python3.12/dist-packages (from llama-stack) (3.1.6)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.12/dist-packages (from llama-stack) (4.25.1)\n",
            "Collecting llama-stack-client>=0.2.22 (from llama-stack)\n",
            "  Downloading llama_stack_client-0.2.22-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: openai>=1.100.0 in /usr/local/lib/python3.12/dist-packages (from llama-stack) (1.107.0)\n",
            "Requirement already satisfied: prompt-toolkit in /usr/local/lib/python3.12/dist-packages (from llama-stack) (3.0.52)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from llama-stack) (1.1.1)\n",
            "Collecting python-jose[cryptography] (from llama-stack)\n",
            "  Downloading python_jose-3.5.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting pydantic>=2.11.9 (from llama-stack)\n",
            "  Downloading pydantic-2.11.9-py3-none-any.whl.metadata (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.4/68.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from llama-stack) (13.9.4)\n",
            "Requirement already satisfied: starlette in /usr/local/lib/python3.12/dist-packages (from llama-stack) (0.47.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from llama-stack) (3.1.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from llama-stack) (0.11.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from llama-stack) (11.3.0)\n",
            "Requirement already satisfied: h11>=0.16.0 in /usr/local/lib/python3.12/dist-packages (from llama-stack) (0.16.0)\n",
            "Requirement already satisfied: python-multipart>=0.0.20 in /usr/local/lib/python3.12/dist-packages (from llama-stack) (0.0.20)\n",
            "Requirement already satisfied: uvicorn>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from llama-stack) (0.35.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.30.0 in /usr/local/lib/python3.12/dist-packages (from llama-stack) (1.36.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http>=1.30.0 (from llama-stack)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.37.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting aiosqlite>=0.21.0 (from llama-stack)\n",
            "  Downloading aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting asyncpg (from llama-stack)\n",
            "  Downloading asyncpg-0.30.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.12/dist-packages (from aiosqlite>=0.21.0->llama-stack) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->llama-stack) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->llama-stack) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->llama-stack) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->llama-stack) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->llama-stack) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->llama-stack) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->llama-stack) (1.1.9)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=3.1.6->llama-stack) (3.0.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-stack-client>=0.2.22->llama-stack) (4.10.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from llama-stack-client>=0.2.22->llama-stack) (8.2.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from llama-stack-client>=0.2.22->llama-stack) (1.9.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from llama-stack-client>=0.2.22->llama-stack) (2.2.2)\n",
            "Collecting pyaml (from llama-stack-client>=0.2.22->llama-stack)\n",
            "  Downloading pyaml-25.7.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from llama-stack-client>=0.2.22->llama-stack) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx->llama-stack) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->llama-stack) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx->llama-stack) (3.10)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.100.0->llama-stack) (0.10.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http>=1.30.0->llama-stack) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-api~=1.15 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http>=1.30.0->llama-stack) (1.36.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.37.0 (from opentelemetry-exporter-otlp-proto-http>=1.30.0->llama-stack)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.37.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.37.0 (from opentelemetry-exporter-otlp-proto-http>=1.30.0->llama-stack)\n",
            "  Downloading opentelemetry_proto-1.37.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.30.0 (from llama-stack)\n",
            "  Downloading opentelemetry_sdk-1.37.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: protobuf<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-proto==1.37.0->opentelemetry-exporter-otlp-proto-http>=1.30.0->llama-stack) (5.29.5)\n",
            "Collecting opentelemetry-api~=1.15 (from opentelemetry-exporter-otlp-proto-http>=1.30.0->llama-stack)\n",
            "  Downloading opentelemetry_api-1.37.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.58b0 (from opentelemetry-sdk>=1.30.0->llama-stack)\n",
            "  Downloading opentelemetry_semantic_conventions-0.58b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api~=1.15->opentelemetry-exporter-otlp-proto-http>=1.30.0->llama-stack) (8.7.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.9->llama-stack) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.9->llama-stack) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.9->llama-stack) (0.4.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->llama-stack) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->llama-stack) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->llama-stack) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->llama-stack) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->llama-stack) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->llama-stack) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->llama-stack) (1.20.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema->llama-stack) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema->llama-stack) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema->llama-stack) (0.27.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit->llama-stack) (0.2.13)\n",
            "Collecting ecdsa!=0.15 (from python-jose[cryptography]->llama-stack)\n",
            "  Downloading ecdsa-0.19.1-py2.py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: rsa!=4.1.1,!=4.4,<5.0,>=4.0 in /usr/local/lib/python3.12/dist-packages (from python-jose[cryptography]->llama-stack) (4.9.1)\n",
            "Requirement already satisfied: pyasn1>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from python-jose[cryptography]->llama-stack) (0.6.1)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.12/dist-packages (from python-jose[cryptography]->llama-stack) (43.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->llama-stack) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->llama-stack) (2.19.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->llama-stack) (2024.11.6)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=3.4.0->python-jose[cryptography]->llama-stack) (2.0.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from ecdsa!=0.15->python-jose[cryptography]->llama-stack) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->llama-stack) (0.1.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0,>=0.34.0->llama-stack) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0,>=0.34.0->llama-stack) (2.5.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas->llama-stack-client>=0.2.22->llama-stack) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->llama-stack-client>=0.2.22->llama-stack) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->llama-stack-client>=0.2.22->llama-stack) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->llama-stack-client>=0.2.22->llama-stack) (2025.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=3.4.0->python-jose[cryptography]->llama-stack) (2.23)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api~=1.15->opentelemetry-exporter-otlp-proto-http>=1.30.0->llama-stack) (3.23.0)\n",
            "Downloading llama_stack-0.2.22-py3-none-any.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m116.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
            "Downloading llama_stack_client-0.2.22-py3-none-any.whl (369 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m369.9/369.9 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_http-1.37.0-py3-none-any.whl (19 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.37.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.37.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.37.0-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.9/131.9 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.37.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.58b0-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.11.9-py3-none-any.whl (444 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.9/444.9 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asyncpg-0.30.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m120.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fire-0.7.1-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.9/115.9 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ecdsa-0.19.1-py2.py3-none-any.whl (150 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.6/150.6 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyaml-25.7.0-py3-none-any.whl (26 kB)\n",
            "Downloading python_jose-3.5.0-py2.py3-none-any.whl (34 kB)\n",
            "Installing collected packages: pyaml, opentelemetry-proto, fire, ecdsa, asyncpg, aiosqlite, python-jose, pydantic, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, opentelemetry-semantic-conventions, llama-stack-client, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-http, llama-stack\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.11.7\n",
            "    Uninstalling pydantic-2.11.7:\n",
            "      Successfully uninstalled pydantic-2.11.7\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.36.0\n",
            "    Uninstalling opentelemetry-api-1.36.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.36.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.57b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.57b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.57b0\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.36.0\n",
            "    Uninstalling opentelemetry-sdk-1.36.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.36.0\n",
            "Successfully installed aiosqlite-0.21.0 asyncpg-0.30.0 ecdsa-0.19.1 fire-0.7.1 llama-stack-0.2.22 llama-stack-client-0.2.22 opentelemetry-api-1.37.0 opentelemetry-exporter-otlp-proto-common-1.37.0 opentelemetry-exporter-otlp-proto-http-1.37.0 opentelemetry-proto-1.37.0 opentelemetry-sdk-1.37.0 opentelemetry-semantic-conventions-0.58b0 pyaml-25.7.0 pydantic-2.11.9 python-jose-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-adamw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zFUrXmtXhb-V",
        "outputId": "ea971551-bbb6-447e-a709-b190e8ead4e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-adamw\n",
            "  Downloading keras_adamw-1.38-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from keras-adamw) (2.0.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (from keras-adamw) (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow->keras-adamw) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow->keras-adamw) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow->keras-adamw) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow->keras-adamw) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow->keras-adamw) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow->keras-adamw) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow->keras-adamw) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow->keras-adamw) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow->keras-adamw) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow->keras-adamw) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow->keras-adamw) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow->keras-adamw) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow->keras-adamw) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow->keras-adamw) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow->keras-adamw) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow->keras-adamw) (1.74.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow->keras-adamw) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow->keras-adamw) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow->keras-adamw) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow->keras-adamw) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow->keras-adamw) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow->keras-adamw) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow->keras-adamw) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow->keras-adamw) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow->keras-adamw) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow->keras-adamw) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow->keras-adamw) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow->keras-adamw) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow->keras-adamw) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow->keras-adamw) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow->keras-adamw) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow->keras-adamw) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow->keras-adamw) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow->keras-adamw) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow->keras-adamw) (0.1.2)\n",
            "Downloading keras_adamw-1.38-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: keras-adamw\n",
            "Successfully installed keras-adamw-1.38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "NEd1gm1ThjLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import BitsAndBytesConfig\n",
        "from transformers import get_scheduler\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import os"
      ],
      "metadata": {
        "id": "5QIpbqy4hki-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    get_scheduler,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from torch.optim import AdamW"
      ],
      "metadata": {
        "id": "D0tN_YLCh_NW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Model and params"
      ],
      "metadata": {
        "id": "hdLUVNVwiWrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBxoAsxCjL81",
        "outputId": "4603466f-2669-425b-f921-0da65b492190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
        "save_dir = \"/content/lora_checkpoints\"\n",
        "os.makedirs(save_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "-ehmCcjuiD-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n",
        "gpu_name = torch.cuda.get_device_name(0)\n",
        "print(f\"Using GPU: {gpu_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZWR94nLiSgr",
        "outputId": "b9a1b6dd-ad82-4984-c18a-736aeade0f3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Sep 20 22:11:25 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0             44W /  400W |       5MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "Using GPU: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if \"A100\" in gpu_name:\n",
        "    device_map = {\"\": 0}  # put everything on GPU\n",
        "else:\n",
        "    device_map = \"balanced_low_0\"  # spread layers GPU+CPU"
      ],
      "metadata": {
        "id": "laj0GTWxiqZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model itself\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    bnb_8bit_use_double_quant=True,\n",
        "    bnb_8bit_quant_type=\"nf4\",\n",
        "    llm_int8_enable_fp32_cpu_offload=True\n",
        ")\n",
        "print(f\"Loading model with device_map={device_map}...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token  # needed for batching"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419,
          "referenced_widgets": [
            "78746cd3e5844e58a1ad62e46ed1cd2b",
            "6383586fbdc54b3cac112371d8a6b760",
            "338124489bb74aadb11498722f7e363b",
            "64108007b42b4b3fbf221f1e5beef5f5",
            "178cb7211e754c53ba6a11ea1bff47c0",
            "174de305e5754ebda9e483d498548203",
            "ede4766af29f46a78e468cadd922273a",
            "b33ac584fcd14fe8865d8381fb138069",
            "eb171230f2024380875cf02b0e799402",
            "8aec0b9851bb404eb6bbdc622f907de1",
            "343168fc1eaf44ab8d743b5aaa8b4148",
            "6d140be450164436b16f43b12b85f0d9",
            "a1a64d6360094476a0ef0564b48908ac",
            "f34bc9027b7c4b99bd43b19bda252095",
            "0e907904e2614962aa936d3408f7a04e",
            "d8baf8e711494592985d9f9a414a3ba7",
            "9462ed9207d440c5b03c7ff98e6ace1b",
            "7ab423869a8a4e25b4505168f371c202",
            "daf4c769893c45b88cd6a7815ff27ab1",
            "552ecad42dec4670b0f90b8adc97594e",
            "1f842e41f80c4b36af3e8cde5bbaadfe",
            "ad7304f451eb48968f87344430b14bb9",
            "88d3ccdf34924d21b7f9057c7a98ba1e",
            "8cfab71664d544d3b0a78070ef93eb02",
            "43a7dcde56fe465a8c25a0eaa41951b3",
            "55c7a51d25d44bdabb342fd92fb16608",
            "c710634929bb4863a6b3341c08337404",
            "3b98cd3fcb6846a0a88a10953607add1",
            "89e6c0b700bb4c5a83423bb290dc3f82",
            "6bce0ee814cc4e5bba9f48ec315f50d9",
            "81744f211eef401a856f3812578108b6",
            "1f198f8ab4ad4de5b8383c337e81482c",
            "fd3339ecd1e34baf9d7f518f6a880af7",
            "4eff26b35cb44143bc28b0fc5594c70a",
            "15f40b0716af4c50b9d294f08063a52f",
            "2b405d29cd894cba80b82b94d29c34f9",
            "a8b591a905bd4b35bb15ef0758ef79a9",
            "410f10f4d02a40cea502393209f9ce9f",
            "7f8163c505284dec9174a5a153124583",
            "355b2b4d04024109a0dd8cce2240d74b",
            "7271048a270c48eaa066bce53e08faff",
            "caf3009d9550477a9ceda5a70a6c6147",
            "578ae6cac7e74af4b1d233a212d8e72e",
            "950c9c7ea5d74b39920a24b8f35f7fe1",
            "2f88a40345c24fe99532f72973f6bca2",
            "ae691c02bf3f49bbbd4077a1ff1eab2a",
            "b95a4f656674424cb71d1d1f71a5d3ac",
            "1d955a8f367441df8c14a42d20c74c83",
            "c80171cc8811418081bceb2f4d503a06",
            "79a7c1a659bc47f5be1c86f43d443a7e",
            "6f6f553d83e84861a4e122ca019a9556",
            "ae572410046749b993d95ad616f92171",
            "0b3a83b7e3ed47d4ac13b57dde075a37",
            "6c3fa57c900b466d91a75cf2b8768919",
            "3c345e41a7e54eebbce5adbb3ef44ddd",
            "5b917fa3bc4d4835b27f8fac9ea73497",
            "c564c07a31804d5db1c89693d99a4ade",
            "20d69d7ffd9e47ac9743cab2929c41f5",
            "4dd7736524974ddf933e757200e9238a",
            "0aae634dc4024910a117b2265b61ec05",
            "9f49aebba826467bb01eedcbb6ed2c61",
            "c7b49bd59adf4e14a81d059173b043c1",
            "aa208ca01e0a4911a6d980c94eb244ea",
            "7b1411cbb5a84744a8e0dfb231dc5873",
            "1f0582998541422691968ab377bb6e20",
            "3a9b1c8a6b764c25bd99ee45af7c4936",
            "5d050925d01249d8bff5f00d23d1aad9",
            "693372fbfe184599b2a217ce9136dd9f",
            "11351081557f48338bfff28f150ba8d4",
            "92ee7d817c354c9c8f9892a8c85682fb",
            "2babffc5de054a25932fc5314f83fec5",
            "4ba463eaeaef4b1689372eee17181d27",
            "4a59b1091aee4a96afda10038a3a7812",
            "ceda72e078ac4686aa771c54fc21d825",
            "77c1e7f649554f539b90cbe5b931faaa",
            "29af208a681746049bf596e4cf3275f0",
            "0cc0c3783d964bd0af15d9ef182a2b1a",
            "ab9e7bc429eb4369894a0ea8a0072f1e",
            "c2ffdac9838843b48a13a58c63388227",
            "03ae7223c1be49c6a7ee3cee13d6d8ee",
            "04c5833adf4f4a3ea8982fa685c76488",
            "0066f94e66524d0c8839c33728574978",
            "b85af4d0908e4d5f9b26b1f88632ae8c",
            "f17e3304cfb2431c912312110793e596",
            "a5ad0038bafe451ca2a61f38ba1265f5",
            "25530c94e45c4154aeb9b9029fda4800",
            "79a53da59eec469eab6ae5e8c737d9e2",
            "ec4a534bf4e94afe83028b8031e029b1",
            "56e0d05acaa441c6babb5092532f7330",
            "c5699fbc2ecf4579a3b5e9259fecfc10",
            "0d85b8813b864a29bf860045c07da788",
            "aa526436092d41c3b683a19abc51b038",
            "d70fdba3c746411cb5a22bcf622ae40d",
            "ff16c05fc14d44348e586e0baaefd384",
            "93322a57882449f391079d2713c970cf",
            "3f99d8d3aa12401e8b2bc25b7079b6ab",
            "78efba8e38d84659b4254d89c8ed5972",
            "b85a98d80a6d41a6aca8f0a0126e84cb",
            "b2305751fad14bdab0d96fed559511bc",
            "cb22c60d42134fa287c78fb4d518fc48",
            "67af5a436e524eaca934cea49aa7c817",
            "dd7ce2b108794bdea01b944d809d7b38",
            "30e94f49f9de4df6bed57ecef5e614f3",
            "139f41ac5a9f40f781caa5a45eea40ea",
            "5a29a6c0542c4696a77078eb1119096b",
            "d789adbad8b4423c99aa66bc6af3fb40",
            "46fd47b9b7d4489885c2b55077cb7ff7",
            "dbe166c55b184030a27425dc72f2cdb1",
            "0a9c5d8b770c4c8a92b16febc63a5d37",
            "f6b66289a1784b5b9dffe588f721c130",
            "0f0044588d6245159b91a384231f67d1",
            "db5d078dcb9d4aefa7eb0b5a71d9156f",
            "4b7ff58a984d40ccb133bc1452e8128e",
            "c8127946a6004b83ad51ee7d2a2d01bb",
            "33696937cb6041d8af7efae2eaa2a8c0",
            "626a7983781c40c29ee5bb8053d48caa",
            "4e3d3dad2bd6438e805fd180a7dcbbe8",
            "c16ddb6679e2482bbf7188ed4b385828",
            "cbe0c71e103440fe84b538c6b73fbfda",
            "3d266fb7e4d64344bfb1419bf33163f1",
            "b70a07b806b14336bab38b7347a96306",
            "4286a9123b7842d2b7a1c0115b14cc19",
            "9098bd9b5dc84fd1bbed18c55184143f",
            "d3dbf048af904e9994fbbcff67652b39",
            "364be69562c64194b0c804cae993536f",
            "5f255b7351b045a7b14f2df971e5b26a",
            "9e3b2520139d4d638b93b81e765affd7",
            "ab66777236134552b0c9de63220670ae",
            "9565425ca0c84a37aa51879dc22f9a39",
            "21452a46ab6f4b8b869ad822e2464f37",
            "b599b3ef455c46a69cbf653befeef450",
            "17015e291de845e2ae34e153e7ffbecd"
          ]
        },
        "id": "YqMTfpBMiY4q",
        "outputId": "8c395501-a673-4b11-cfad-564b7f19db76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model with device_map={'': 0}...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "78746cd3e5844e58a1ad62e46ed1cd2b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6d140be450164436b16f43b12b85f0d9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88d3ccdf34924d21b7f9057c7a98ba1e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4eff26b35cb44143bc28b0fc5594c70a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f88a40345c24fe99532f72973f6bca2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b917fa3bc4d4835b27f8fac9ea73497"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5d050925d01249d8bff5f00d23d1aad9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab9e7bc429eb4369894a0ea8a0072f1e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "56e0d05acaa441c6babb5092532f7330"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cb22c60d42134fa287c78fb4d518fc48"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f0044588d6245159b91a384231f67d1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4286a9123b7842d2b7a1c0115b14cc19"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# freeze base model\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "7zMOrad3iwdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lora config\n"
      ],
      "metadata": {
        "id": "xWqCU6CjiVUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# LoRA config\n",
        "# -----------------------\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        ")\n",
        "\n",
        "print(\"Applying LoRA adapters...\")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7dxxn-SjJlI",
        "outputId": "4f065008-78cd-44d9-8d33-12ddcca31d0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying LoRA adapters...\n",
            "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "kbdKJVuKjb5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#V2"
      ],
      "metadata": {
        "id": "gtyJGRMfM_9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'base_initial_model.pt')"
      ],
      "metadata": {
        "id": "IQm_KMliO_y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_loader))\n",
        "print(batch.keys())\n",
        "print(batch[\"input_ids\"].shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQGM4J1cQUww",
        "outputId": "6a19c590-14a8-4d0d-e9dd-f643c8b98ab0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KeysView({'input_ids': tensor([[128000,  20421,    220,   3174,    198,  19285],\n",
            "        [128000,  19285,    198,  19285, 128001, 128001]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 0, 0]]), 'labels': None})\n",
            "torch.Size([2, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "csv_path = \"/content/new_all_podcasts.csv\"\n",
        "output_file = \"file_clean.csv\"\n",
        "\n",
        "good_rows = []\n",
        "\n",
        "with open(csv_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    header = f.readline()\n",
        "    good_rows.append(header)\n",
        "    for line in f:\n",
        "        try:\n",
        "            if len(line.strip().split(\",\")) < 2:\n",
        "                continue\n",
        "            good_rows.append(line.replace('\"', ''))\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.writelines(good_rows)\n",
        "\n",
        "df = pd.read_csv(output_file, on_bad_lines=\"skip\")\n",
        "print(\"Rows after cleaning:\", len(df))\n",
        "print(df.head())\n",
        "\n",
        "# -----------------------\n",
        "# 2️⃣ Tokenizer + chunking\n",
        "# -----------------------\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # causal LM doesn’t have a pad token\n",
        "\n",
        "max_tokens = 256\n",
        "stride = 50\n",
        "\n",
        "def chunk_text(text, max_tokens=max_tokens, stride=stride):\n",
        "    tokens = tokenizer.encode(text)\n",
        "    chunks = []\n",
        "    for i in range(0, len(tokens), max_tokens - stride):\n",
        "        chunk_tokens = tokens[i:i + max_tokens]\n",
        "        if not chunk_tokens:\n",
        "            continue\n",
        "        chunks.append(chunk_tokens)\n",
        "        if i + max_tokens >= len(tokens):\n",
        "            break\n",
        "    return chunks\n",
        "\n",
        "rows = []\n",
        "for idx, row in df.iterrows():\n",
        "    title = str(row[\"Title\"])\n",
        "    transcript = str(row[\"Transcript\"])\n",
        "    token_chunks = chunk_text(transcript)\n",
        "    for chunk_tokens in token_chunks:\n",
        "        # combine title + chunk tokens as text\n",
        "        text = f\"{title}\\n{tokenizer.decode(chunk_tokens, skip_special_tokens=True)}\"\n",
        "        rows.append({\"text\": text})\n",
        "\n",
        "# -----------------------\n",
        "# 3️⃣ Hugging Face Dataset\n",
        "# -----------------------\n",
        "dataset = Dataset.from_list(rows)\n",
        "\n",
        "def tokenize_fn(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_tokens\n",
        "    )\n",
        "\n",
        "dataset = dataset.map(tokenize_fn, batched=True)\n",
        "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "print(\"Dataset ready! Number of examples:\", len(dataset))\n",
        "\n",
        "# -----------------------\n",
        "# 4️⃣ DataLoader for causal LM\n",
        "# -----------------------\n",
        "batch_size = 2 if torch.cuda.is_available() else 1\n",
        "collate_fn = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # causal LM\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 981,
          "referenced_widgets": [
            "0ade93553b474268a5ef137d744402e8",
            "3f2740f6cbfd477586b06a01dc47048d",
            "da8bd3d966e64d13a057d7fe93d745a0",
            "b4b643ca76b14e999e375aaef5ee2eb0",
            "ef8a8f56b9cc4fb0b1da5fdb844af830",
            "9f9123fa5ef94b829297a3dcc6d39c94",
            "dcf95bbd8b4c47e4bfb72cfae46e69f4",
            "73a394c9e98d46a89e90848fb3acc2c2",
            "05138fdd70b44c25a984150de5727a63",
            "faab58da904a48dbbd8cdbe9ca62290e",
            "4d7722a0265b4f64854a82f6bcaa6679"
          ]
        },
        "id": "5JoTPxX9M_iv",
        "outputId": "42d1027d-e193-45e5-f581-e963aff8dc9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows after cleaning: 754\n",
            "                                                                                                                                                                                                                                                                                                                                 Title  \\\n",
            "Persepolis                                         Sleepy History                      Sun      27 Apr 2025 16:30:00 -0000 Persepolis—once a dazzling jewel of the ancient...  filled with towering columns                      intricate carvings                                   and the whispers of kings. Built to impress a...   \n",
            "Explore history's most intriguing stories           people                              places  events                      and mysteries                                      delivered in a supremely calming atmosphere. I... Sleepy History is the perfect bedtime companion...    pulling your mind away from any racing thoughts   \n",
            "Sleepy History is a production of Slumber Studi...  visit ⁠⁠⁠www.slumberstudios.com⁠⁠⁠ NaN     NaN                         NaN                                                NaN                                                NaN                                                                                               NaN   \n",
            "Cicely Mary Barker                                 Sleepy History                      Sun      20 Apr 2025 16:30:00 -0000 Cicely Mary Barker—an artist whose delicate bru...  she blended botanical beauty with whimsical charm captivating hearts young and old. But who was t...                       and what inspired her gentle   \n",
            "Explore history's most intriguing stories           people                              places  events                      and mysteries                                      delivered in a supremely calming atmosphere. I... Sleepy History is the perfect bedtime companion...    pulling your mind away from any racing thoughts   \n",
            "\n",
            "                                                                                                                                                                                                                                                                                                                               Podcast  \\\n",
            "Persepolis                                         Sleepy History                      Sun      27 Apr 2025 16:30:00 -0000 Persepolis—once a dazzling jewel of the ancient...  filled with towering columns                      intricate carvings                                                      it stood as a symbol of power   \n",
            "Explore history's most intriguing stories           people                              places  events                      and mysteries                                      delivered in a supremely calming atmosphere. I... Sleepy History is the perfect bedtime companion...   making room for the soothing music and calmin...   \n",
            "Sleepy History is a production of Slumber Studi...  visit ⁠⁠⁠www.slumberstudios.com⁠⁠⁠ NaN     NaN                         NaN                                                NaN                                                NaN                                                                                               NaN   \n",
            "Cicely Mary Barker                                 Sleepy History                      Sun      20 Apr 2025 16:30:00 -0000 Cicely Mary Barker—an artist whose delicate bru...  she blended botanical beauty with whimsical charm captivating hearts young and old. But who was t...                         imaginative world? Tonight   \n",
            "Explore history's most intriguing stories           people                              places  events                      and mysteries                                      delivered in a supremely calming atmosphere. I... Sleepy History is the perfect bedtime companion...   making room for the soothing music and calmin...   \n",
            "\n",
            "                                                                                                                                                                                                                                                                                                                          Pubdate  \\\n",
            "Persepolis                                         Sleepy History                      Sun      27 Apr 2025 16:30:00 -0000 Persepolis—once a dazzling jewel of the ancient...  filled with towering columns                      intricate carvings                                                                       culture   \n",
            "Explore history's most intriguing stories           people                              places  events                      and mysteries                                      delivered in a supremely calming atmosphere. I... Sleepy History is the perfect bedtime companion...                                           NaN   \n",
            "Sleepy History is a production of Slumber Studi...  visit ⁠⁠⁠www.slumberstudios.com⁠⁠⁠ NaN     NaN                         NaN                                                NaN                                                NaN                                                                                          NaN   \n",
            "Cicely Mary Barker                                 Sleepy History                      Sun      20 Apr 2025 16:30:00 -0000 Cicely Mary Barker—an artist whose delicate bru...  she blended botanical beauty with whimsical charm captivating hearts young and old. But who was t...   step into Cicely Mary Barker’s life and art   \n",
            "Explore history's most intriguing stories           people                              places  events                      and mysteries                                      delivered in a supremely calming atmosphere. I... Sleepy History is the perfect bedtime companion...                                           NaN   \n",
            "\n",
            "                                                                                                                                                                                                                                                                                                                               Content  \\\n",
            "Persepolis                                         Sleepy History                      Sun      27 Apr 2025 16:30:00 -0000 Persepolis—once a dazzling jewel of the ancient...  filled with towering columns                      intricate carvings                                   and ceremony. But what stories lie within its...   \n",
            "Explore history's most intriguing stories           people                              places  events                      and mysteries                                      delivered in a supremely calming atmosphere. I... Sleepy History is the perfect bedtime companion...                                                NaN   \n",
            "Sleepy History is a production of Slumber Studi...  visit ⁠⁠⁠www.slumberstudios.com⁠⁠⁠ NaN     NaN                         NaN                                                NaN                                                NaN                                                                                               NaN   \n",
            "Cicely Mary Barker                                 Sleepy History                      Sun      20 Apr 2025 16:30:00 -0000 Cicely Mary Barker—an artist whose delicate bru...  she blended botanical beauty with whimsical charm captivating hearts young and old. But who was t...                   as her story blossoms around you   \n",
            "Explore history's most intriguing stories           people                              places  events                      and mysteries                                      delivered in a supremely calming atmosphere. I... Sleepy History is the perfect bedtime companion...                                                NaN   \n",
            "\n",
            "                                                                                                                                                                                                                                                                                                                    Link  \\\n",
            "Persepolis                                         Sleepy History                      Sun      27 Apr 2025 16:30:00 -0000 Persepolis—once a dazzling jewel of the ancient...  filled with towering columns                      intricate carvings                                    and what led to its fall? Tonight   \n",
            "Explore history's most intriguing stories           people                              places  events                      and mysteries                                      delivered in a supremely calming atmosphere. I... Sleepy History is the perfect bedtime companion...                                  NaN   \n",
            "Sleepy History is a production of Slumber Studi...  visit ⁠⁠⁠www.slumberstudios.com⁠⁠⁠ NaN     NaN                         NaN                                                NaN                                                NaN                                                                                 NaN   \n",
            "Cicely Mary Barker                                 Sleepy History                      Sun      20 Apr 2025 16:30:00 -0000 Cicely Mary Barker—an artist whose delicate bru...  she blended botanical beauty with whimsical charm captivating hearts young and old. But who was t...   guiding you into a peaceful sleep.   \n",
            "Explore history's most intriguing stories           people                              places  events                      and mysteries                                      delivered in a supremely calming atmosphere. I... Sleepy History is the perfect bedtime companion...                                  NaN   \n",
            "\n",
            "                                                                                                                                                                                                                                                                                                                              AudioURL  \\\n",
            "Persepolis                                         Sleepy History                      Sun      27 Apr 2025 16:30:00 -0000 Persepolis—once a dazzling jewel of the ancient...  filled with towering columns                      intricate carvings                                   travel back to the grandeur and mystery of Pe...   \n",
            "Explore history's most intriguing stories           people                              places  events                      and mysteries                                      delivered in a supremely calming atmosphere. I... Sleepy History is the perfect bedtime companion...                                                NaN   \n",
            "Sleepy History is a production of Slumber Studi...  visit ⁠⁠⁠www.slumberstudios.com⁠⁠⁠ NaN     NaN                         NaN                                                NaN                                                NaN                                                                                               NaN   \n",
            "Cicely Mary Barker                                 Sleepy History                      Sun      20 Apr 2025 16:30:00 -0000 Cicely Mary Barker—an artist whose delicate bru...  she blended botanical beauty with whimsical charm captivating hearts young and old. But who was t...                                                NaN   \n",
            "Explore history's most intriguing stories           people                              places  events                      and mysteries                                      delivered in a supremely calming atmosphere. I... Sleepy History is the perfect bedtime companion...                                                NaN   \n",
            "\n",
            "                                                                                                                                                                                                                                                                                                                            Transcript  \n",
            "Persepolis                                         Sleepy History                      Sun      27 Apr 2025 16:30:00 -0000 Persepolis—once a dazzling jewel of the ancient...  filled with towering columns                      intricate carvings                                   as echoes from the past gently guide you into...  \n",
            "Explore history's most intriguing stories           people                              places  events                      and mysteries                                      delivered in a supremely calming atmosphere. I... Sleepy History is the perfect bedtime companion...                                                NaN  \n",
            "Sleepy History is a production of Slumber Studi...  visit ⁠⁠⁠www.slumberstudios.com⁠⁠⁠ NaN     NaN                         NaN                                                NaN                                                NaN                                                                                               NaN  \n",
            "Cicely Mary Barker                                 Sleepy History                      Sun      20 Apr 2025 16:30:00 -0000 Cicely Mary Barker—an artist whose delicate bru...  she blended botanical beauty with whimsical charm captivating hearts young and old. But who was t...                                                NaN  \n",
            "Explore history's most intriguing stories           people                              places  events                      and mysteries                                      delivered in a supremely calming atmosphere. I... Sleepy History is the perfect bedtime companion...                                                NaN  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/754 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ade93553b474268a5ef137d744402e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset ready! Number of examples: 754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "all_files = glob.glob(os.path.join(r'/content', '*.csv'))\n",
        "df = pd.concat((pd.read_csv(f) for f in all_files), ignore_index=True)\n",
        "\n",
        "df.to_csv(f'new_all_podcasts.csv', index=False)"
      ],
      "metadata": {
        "id": "EHtHA5l4Md3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "just_in_case = copy.deepcopy(dataset)"
      ],
      "metadata": {
        "id": "VxUh5_wrmUlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizer and scheduler"
      ],
      "metadata": {
        "id": "Zmwp16oHm8ds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3B3SbSGpH_D",
        "outputId": "bb21824a-af8d-450f-b8a6-077eb35d42ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([128000,  97012,    323,  34944,    320,   7560,    461,    340,    220,\n",
            "          2366,     19]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-4)\n",
        "num_epochs = 10\n",
        "num_training_steps = num_epochs * len(train_loader)\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "MYAAYjebm8B7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test function"
      ],
      "metadata": {
        "id": "IjlegKCTnAO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, tokenizer, prompt=\"Cats in trees\", max_new_tokens=500, device='cuda'):\n",
        "    model.eval()\n",
        "\n",
        "    # Make sure pad_token is set\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            eos_token_id=tokenizer.eos_token_id,  # stop generation at EOS\n",
        "            pad_token_id=tokenizer.pad_token_id,  # treat PAD safely\n",
        "            do_sample=True,        # optional, adds randomness for more “natural” output\n",
        "            top_p=0.9,             # nucleus sampling\n",
        "            temperature=0.7        # controls randomness\n",
        "        )\n",
        "\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return decoded"
      ],
      "metadata": {
        "id": "Yjdwg8hFNN59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = test_model(model, tokenizer, prompt=\"Boring information about puppies\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRdl964COT5P",
        "outputId": "6449924d-d74e-448b-af29-24c24142f1da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Boring information about puppies\n",
            "What a lot of people don’t know about puppies is that they are born without much of the skills and knowledge that we take for granted in adult dogs. In fact, the only thing they can do is eat and sleep. They have no idea how to open their eyes, or walk, or run, or play, or even bark. It’s amazing that they can even survive on their own.\n",
            "I have seen some puppies that are so small that they can’t even stand on their own. They are so small that they can’t even see. They have to rely on their mother’s instincts to guide them. They can’t even see their mother. They are so small that they have to rely on their mother’s instincts to guide them.\n",
            "This is why it is so important to be careful about what you feed your puppy. You should be careful about what you feed your puppy, because you dont want your puppy to grow up to be a big, fat, lazy, stupid puppy. You want your puppy to be able to stand on its own and walk around.\n",
            "We all know that puppies are not born with a fully developed brain. They are born with a brain that is still developing. This is why it is so important to be careful about what you feed your puppy. You should be careful about what you feed your puppy, because you dont want your puppy to grow up to be a big, fat, lazy, stupid puppy. You want your puppy to be able to stand on its own and walk around.\n",
            "There are a few things that you can do to help your puppy grow up to be a big, fat, lazy, stupid puppy. You can feed your puppy a diet that is low in carbohydrates and high in protein. You can also feed your puppy a diet that is low in fat and high in protein. You can also feed your puppy a diet that is low in calories and high in protein. You can also feed your puppy a diet that is low in calories and high in protein.\n",
            "This is a good point, but it is also true that you can feed your puppy a diet that is high in protein and low in fat and low in calories. The best way to do this is to feed your puppy a diet that is low in fat and high in protein and low in calories. The best way to do this is to feed your puppy a diet that is high in protein and low in fat and low in calories.\n",
            "A lot of people think that the best way to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = test_model(model, tokenizer, prompt=\"I will tell you about puppies\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0uIctSKN0Yn",
        "outputId": "d0e0625f-b2e1-41cc-8d05-2ff6d0569b0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I will tell you about puppies who are born and raised in our family. We have 3 dogs and 6 puppies. The puppies are born on February 7, 2015 and we have 4 males and 2 females. The mother is a German Shepherd Dog and the father is a American Cocker Spaniel. The puppies are 4 months old and they have a great character and are very playful. They are very good with people and other dogs and they love to play. They are very good with kids and they are very friendly. They are very good with people and other dogs and they love to play. They are very good with kids and they are very friendly. They are very good with people and other dogs and they love to play. They are very good with kids and they are very friendly. They are very good with people and other dogs and they love to play. They are very good with kids and they are very friendly. They are very good with people and other dogs and they love to play. They are very good with kids and they are very friendly. They are very good with people and other dogs and they love to play. They are very good with kids and they are very friendly. They are very good with people and other dogs and they love to play. They are very good with kids and they are very friendly. They are very good with people and other dogs and they love to play. They are very good with kids and they are very friendly. They are very good with people and other dogs and they love to play. They are very good with kids and they are very friendly. They are very good with people and other dogs and they love to play. They are very good with kids and they are very friendly. They are very good with people and other dogs and they love to play. They are very good with kids and they are very friendly. They are very good with people and other dogs and they love to play. They are very good with kids and they are very friendly. They are very good with people and other dogs and they love to play. They are very good with kids and they are very friendly. They are very good with people and other dogs and they love to play. They are very good with kids and they are very friendly. They are very good with people and other dogs and they love to play. They are very good with kids and they are very friendly. They are very good with people and other dogs and they love to play. They are\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = test_model(model, tokenizer, prompt=\"Here is a long boring story about puppies\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "il3wF6zKU3AB",
        "outputId": "ef8c69dd-33ab-4dc7-e549-bc3ed1a4f2d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is a long boring story about puppies. It is the story of how I found my puppy, and it is a long and boring story. But I will tell it anyway.\n",
            "I was walking through a dog park in the city, and I saw a little puppy sitting on the grass, watching the sun go down. I couldn’t believe it was real. I was so excited to find my puppy. I had been searching for a puppy for a long time, and I finally found one.\n",
            "It was a long and boring story, but it was a good story. It was about a puppy named Buddy who was born with a rare condition that made him sleep for hours at a time. He was so excited to find his puppy that he didn’t care that it was a long and boring story. He was so excited to find his puppy that he didn’t care that it was a long and boring story. He was so excited to find his puppy that he didn’t care that it was a long and boring story.\n",
            "Buddy was born with a rare condition that made him sleep for hours at a time. He was so excited to find his puppy that he didn’t care that it was a long and boring story. He was so excited to find his puppy that he didn’t care that it was a long and boring story. He was so excited to find his puppy that he didn’t care that it was a long and boring story.\n",
            "The story of Buddy is told in a way that makes you feel like you’re watching a movie. It’s a story that’s so simple and beautiful that it makes you feel like you’re watching a movie. It’s a story that’s so simple and beautiful that it makes you feel like you’re watching a movie. It’s a story that’s so simple and beautiful that it makes you feel like you’re watching a movie.\n",
            "Buddy is a puppy that has been living with his owner, but he is not the only dog in the story. The other dog in the story is a black lab named Bucky. Bucky is a puppy that has been living with his owner, but he is not the only dog in the story. The other dog in the story is a black lab named Bucky. Bucky is a puppy that has been living with his owner, but he is not the only dog in the story.\n",
            "Bucky is a puppy that has been living with his owner, but he is not the only dog in the story. The other dog in the story is a black lab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = test_model(model, tokenizer, prompt=\"Welcome to my long boring podcast, todays episode is about puppies\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3C9EuBMUhdn",
        "outputId": "5f30c5dd-9928-4307-e0ce-867b9c695698"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to my long boring podcast, todays episode is about puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies and puppies\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = test_model(model, tokenizer, prompt=\"I will tell you about puppies\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gM_gxL6T8-p",
        "outputId": "10ee5f57-ea57-4d2d-e60e-56036e2c65e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I will tell you about puppies and other things, but first I must tell you about my trip to the dog park.\n",
            "We have been going to the dog park for a few weeks now. We go in the early morning, before the sun is up, and we go in the evening, when the sun is just going down. We go when it is cold and we go when it is warm. We go when the weather is bad and we go when the weather is good.\n",
            "We go to the dog park, because I love to watch my dog play. I love to watch her run and jump and chase after other dogs. I love to watch her dig holes and bury bones. I love to watch her run and jump and chase after other dogs.\n",
            "I have been going to the dog park for a few weeks now. I have been going to the dog park for a few weeks now. I have been going to the dog park for a few weeks now.\n",
            "I love to watch her run and jump and chase after other dogs. I love to watch her dig holes and bury bones. I love to watch her run and jump and chase after other dogs.\n",
            "The dog park is a great place to watch dogs play. It’s a great place to watch dogs play. It’s a great place to watch dogs play.\n",
            "I love to watch her run and jump and chase after other dogs. I love to watch her dig holes and bury bones. I love to watch her run and jump and chase after other dogs. I love to watch her run and jump and chase after other dogs. I love to watch her run and jump and chase after other dogs. I love to watch her run and jump and chase after other dogs. I love to watch her run and jump and chase after other dogs.\n",
            "I love to watch her run and jump and chase after other dogs. I love to watch her dig holes and bury bones. I love to watch her run and jump and chase after other dogs. I love to watch her run and jump and chase after other dogs. I love to watch her run and jump and chase after other dogs. I love to watch her run and jump and chase after other dogs. I love to watch her run and jump and chase after other dogs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = test_model(model, tokenizer, prompt=\"Welcome to tonight’s podcast to help you sleep, about puppies\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xP-cy-fLaHDW",
        "outputId": "beffa58f-9584-4292-99f9-230d7d564a5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to tonight’s podcast to help you sleep, about puppies. The puppy stage is the best stage of life. A puppy will bring you joy and happiness. A puppy will give you unconditional love. A puppy will be your best friend. But, a puppy is also a lot of work. And, if you’re not ready for the commitment, a puppy is not for you.\n",
            "The podcast will start with a brief introduction to puppies and the benefits they can bring to your life. It will then go into the stages of puppyhood, from the cute and cuddly to the naughty and destructive. It will also discuss the importance of training and socialization, as well as the joys of watching your puppy grow up.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = test_model(model, tokenizer, prompt=\"Welcome to tonight’s podcast to help you sleep, about puppies\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ez5dA3wdNKmD",
        "outputId": "baba80e7-a060-4d0d-e8eb-abaae22dcd57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to tonight’s podcast to help you sleep, about puppies. I’m not talking about the cute ones that will be all over the internet this weekend, but the kind of puppies that are still in the womb. I’m talking about the puppies that have not been born yet.\n",
            "Now, I know what you’re thinking. What does this have to do with sleep? Well, the answer is that we can learn a lot about sleep by studying the development of puppies in the womb. Let’s take a look at what we know so far.\n",
            "First, it’s important to note that puppies are not born with the ability to sleep. In fact, they don’t even have a sleep/wake cycle until they are about three weeks old. That’s when they start to open their eyes and move around a bit. But even then, they are not very good at sleeping.\n",
            "Puppies in the womb are very active, moving around a lot and practicing breathing. They also have a lot of energy, and they are very curious. All of this activity can make it difficult for them to fall asleep.\n",
            "But as they get older, puppies start to calm down and their sleep/wake cycle becomes more stable. They also start to develop a sleep/wake cycle of their own. This means that they start to sleep at night and be awake during the day.\n",
            "Now, I know what you’re thinking. How does this help us sleep better? Well, by understanding how puppies develop in the womb, we can learn how to create a more stable sleep/wake cycle for ourselves. We can also learn how to make sure that we are getting enough sleep each night.\n",
            "So, if you’re looking for a way to improve your sleep, I suggest taking a look at the development of puppies in the womb. It just might help you get a better night’s sleep.\n",
            "1 Why is it so hard for puppies to sleep?\n",
            "2 How long does a puppy need to sleep?\n",
            "3 What is the sleep cycle of a puppy?\n",
            "4 How do puppies sleep at night?\n",
            "5 Do puppies sleep with their eyes open?\n",
            "6 Do puppies sleep 24 hours a day?\n",
            "7 Why do puppies sleep so much?\n",
            "Why is it so hard for puppies to sleep?\n",
            "It’s no secret that puppies can be a handful. They’re full of energy and seem to have boundless curiosity. But one thing that can be particularly challenging for puppies is getting them to sleep.\n",
            "There are a few reasons why puppies may have trouble sleeping. For one, they’re used to being active and may not\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, prompt=\"Cats in trees\"):\n",
        "    model.eval()\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=500)\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return decoded"
      ],
      "metadata": {
        "id": "TpTAf8wJnBh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_model(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "T1s6MQIdnj09",
        "outputId": "34b7a236-8453-461a-907c-0fd2255f4133"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Cats in trees are a common sight, but when they are up high, it can be hard to reach them. If you are looking for a way to get your cat down from a tree, you can use a cat tree ladder. A cat tree ladder is a ladder that is specifically designed to help you reach your cat when they are up high. This ladder is made with a wide base so that it will not tip over, and it has steps that are spaced far apart so that your cat can easily climb up and down.\\nHow To Get A Cat Down From A Tree\\nThere are a few ways to get a cat down from a tree. One way is to use a ladder. Another way is to use a cat tree ladder.\\n– A cat tree ladder – A cat tree – A ladder\\nIf the cat is high up in the tree, you may need to use a ladder to reach it\\nGet a cat tree ladder\\nThe cat will likely be scared and will not want to come down\\nIf the cat is on a low branch, you can try to coax it down with food or a toy\\n– If the cat is on a low branch, you can try to coax it down with food or a toy. – If the cat is on a high branch, you may need to use a ladder or a tree ladder to get it down. – If the cat is in a tree that is too high to reach, you may need to call a professional to help you get it down.\\nHow To Get A Cat Down From A Tree?\\nHow To Get A Cat Down From A Tree\\nThere are a few ways to get a cat down from a tree. One way is to use a ladder. Another way is to use a cat tree ladder.\\nHow To Get A Cat Down From A Tree?\\nThere are a few ways to get a cat down from a tree. One way is to use a ladder. Another way is to use a cat tree ladder.\\nHow To Get A Cat Down From A Tree?\\nThere are a few ways to get a cat down from a tree. One way is to use a ladder. Another way is to use a cat tree ladder.\\nHow To Get A Cat Down From A Tree?\\nThere are a few ways to get a cat down from a tree. One way is to use a ladder. Another way is to use a cat tree ladder.\\nHow To Get A Cat Down From A Tree?\\nThere are a few ways to get a cat down from a tree'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LoRa fine tuning"
      ],
      "metadata": {
        "id": "ipd27ox2owZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting LoRA fine-tuning...\")\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
        "    for step, batch in enumerate(progress_bar):\n",
        "        model.train()\n",
        "        # print(batch.items())\n",
        "        batch = {k: v.to(device) for k, v in batch.items() if k in [\"input_ids\", \"attention_mask\"]}\n",
        "\n",
        "        outputs = model(**batch, labels=batch[\"input_ids\"])\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        completed_steps = epoch * len(train_loader) + step + 1\n",
        "        total_steps = num_epochs * len(train_loader)\n",
        "        eta = elapsed / completed_steps * (total_steps - completed_steps)\n",
        "        progress_bar.set_postfix({\"loss\": loss.item(), \"ETA (s)\": int(eta)})\n",
        "\n",
        "        # Save checkpoint every 100 steps\n",
        "        if step % 300 == 0:\n",
        "            checkpoint_path = os.path.join(save_dir, f\"checkpoint_step_{completed_steps}.pt\")\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "            checkpoint_path = os.path.join(save_dir, f\"lora_adapter_step_{completed_steps}\")\n",
        "            model.save_pretrained(checkpoint_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "cLWTs-rlowAI",
        "outputId": "e9e7db60-3636-4105-dfb9-e49bff64e95a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting LoRA fine-tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 377/377 [03:15<00:00,  1.93it/s, loss=0.00162, ETA (s)=1758]\n",
            "Epoch 2:   0%|          | 0/377 [00:01<?, ?it/s, loss=0.00117, ETA (s)=1756]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-876420510.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m300\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mcheckpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"checkpoint_step_{completed_steps}.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    965\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m             _save(\n\u001b[0m\u001b[1;32m    968\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m                 \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1264\u001b[0m                     \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1266\u001b[0;31m                     \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m             \u001b[0;31m# Now that it is on the CPU we can directly copy it into the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m             \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/storage.py\u001b[0m in \u001b[0;36mcpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;34m\"\"\"Return a CPU copy of this storage if it's not already on the CPU.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUntypedStorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vlogWlHPunO",
        "outputId": "27a80ea8-f74a-40a8-ece4-eaff1f157b43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([128000,    323,    279,  89248,    315,  45619,     13,  34154,    311,\n",
            "         10098,    323,  31740,    198,    439,  71057,    505,    279,   3347,\n",
            "         30373,   8641,    499,   1139,    264,  26733,    323,  58105,   6212,\n",
            "            13, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "prompt = \"Once upon a time\"\n",
        "print(\"Test output after training:\", test_model(model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iy5ZZwQOyOLk",
        "outputId": "adbe0771-6fb0-4b30-e20b-e0ec7f21e525"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test output after training: Cats in trees (Encore)\n",
            "nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "prompt = \"Once upon a time\"\n",
        "print(\"Test output after training:\", test_model(model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5E_pXaiq9I4",
        "outputId": "7fdbb499-f2aa-4dd0-a622-0e6aeb887bcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test output after training: Cats in trees (Encore)\n",
            "nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# Save final LoRA model\n",
        "# -----------------------\n",
        "final_dir = os.path.join(save_dir, \"final_lora_model\")\n",
        "model.save_pretrained(final_dir)\n",
        "tokenizer.save_pretrained(final_dir)\n",
        "print(f\"Model saved to {final_dir}\")"
      ],
      "metadata": {
        "id": "m6xRCZ_Qq8RS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading it back"
      ],
      "metadata": {
        "id": "7U9Kmf-7XegE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "from peft import PeftModel"
      ],
      "metadata": {
        "id": "AlOmMpP8Xmmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model2 = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "model2 = PeftModel.from_pretrained(base_model2, \"/content/lora_checkpoints/lora_adapter_step_test\")\n",
        "model2.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "96f55803918840f6bbf81294bfb25654",
            "b8d1beadbdf24802a222b522bb5b2af3",
            "cc82bede5bd0492c8e60f8fcc51f86c3",
            "5bef82b9a5e84aa4a31e4b0427ec8e9b",
            "5d2fff7849ad4087b64963bf016d8e41",
            "e8bb9f579acb4ca0b2c084facfec05d0",
            "76927795fe864d309f0b5f34b4384300",
            "907189f81c0940d3b6318ce2ecd02c52",
            "c3083a40520944a7b18ad332b449f6eb",
            "ec1d2ceaa89e409a8b2da92611b00370",
            "b6461fd6070c46fe99ac8946da88c680"
          ]
        },
        "id": "eExCGgZCXgI4",
        "outputId": "eede695b-9f03-4870-a982-b1adff34f76e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "96f55803918840f6bbf81294bfb25654"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LlamaForCausalLM(\n",
              "      (model): LlamaModel(\n",
              "        (embed_tokens): Embedding(128256, 4096)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x LlamaDecoderLayer(\n",
              "            (self_attn): LlamaAttention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "        (rotary_emb): LlamaRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = os.path.join(save_dir, f\"lora_adapter_step_test\")\n",
        "model.save_pretrained(checkpoint_path)"
      ],
      "metadata": {
        "id": "pHhsq84UXIM-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
